## so 라이브러리란

`.so` 파일은 Linux 및 UNIX 시스템에서 사용되는 공유 라이브러리(Shared Object) 파일을 의미한다. 공유 라이브러리는 다양한 프로그램에서 공유하여 사용할 수 있는 코드와 데이터를 포함하고 있어, 메모리 사용을 효율적으로 만들고, 디스크 공간을 절약할 수 있다.

### 주요 특징

- **재사용성**: 같은 라이브러리를 다른 여러 프로그램에 동시에 사용할 수 ㅣㅇㅆ다.
- **메모리 효율성**: 하나의 `.so` 파일이 시스템 메모리에 한 번만 로드되고, 여러 프로그램이 동일한 메모리 영역을 공유하여 사용할 수 있다. 이는 각 프로그램이 자체적으로 라이브러리의 복사본을 메모리에 로드하는 것보다 훨씬 효율적이다.
- **디스크 공간 절약**: 한 번만 디스크에 저장되며, 여러 프로그램에서 이를 공유하므로 각 프로그램별로 중복 저장할 필요가 없다.
- **동적 링킹**: 프로그램이 실행될 때 필요한 `.so` 파일들이 동적으로 로드된다.
- **업데이트의 용이성**: 공유 라이브러리를 업데이트하면, 해당 라이브러리를 사용하는 모든 프로그램이 자동으로 최신 라이브러리의 이점을 누릴 수 있다. (별도의 업데이트할 필요 x)

## ONNX란

- Open Neural Network Exchange의 약자로, 딥러닝 모델을 서로 다른 프레임워크 간에 서로 옮길 수 있도록 하는 오픈 소스 프로젝트이다. ONNX는 모델을 중간 계층 형식으로 변환하고 모델을 실행하는 데 필요한 라이브러리를 제공한다.
- 즉, 다양한 플랫폼 환경(java, c, c++, c#)에서 환경에 제약 없이 구현된 ‘ML 모델’을 호출하고 수행하여 수행 결과값을 반환받는 것을 의미합니다.

### ONNX Runtime 이란

- ONNX 모델을 실행하기 위한 엔진입니다. ONNX 모델을 실행하기 위해 ONNX 런타임은 빠른 추론을 위한 최적화된 커널을 사용합니다.
- 또한, ONNX Runtime은 CPU, GPU 및 딥러닝 가속기(DNNL, NNAPI, OpenVINO)를 지원합니다.
- 따라서, ONNX Runtime은 ONNX 모델을 실행하기 위한 최적화된 런타임 환경을 제공합니다.

### ONNX의 동작원리

<img width="891" alt="onnx" src="https://github.com/devFancy/memo/assets/83820185/4db43cc3-431f-46f8-92d3-424b0cc6a7d0">

1. 모델을 생성한다.
   - 다양한 모델을 ONNX로 변환하여 모델을 생성
2. 사용하려는 환경에서 생성한 모델을 로드하고 실행한다.
   - 환경은 python, c++, c#, c, java, obj-c 환경을 지원한다.
3. (선택) 다양한 런타임 구성 또는 하드웨어 가속기를 사용하여 성능을 조정한다.
4. 입력 값을 전달하여 결과 값을 반환받는다.

**참고자료**

- https://adjh54.tistory.com/203
- https://onnx.ai/onnx/intro/concepts.html
